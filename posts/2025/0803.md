---
title: 清凤的数字分身
tags: [AI]
categories: [项目]
date: 2025-08-03
description: 这是一个数字分身项目利用 QQ 的 C2C 聊天记录作为数据集，对大模型进行微调，实现个性化的数字分身。
articleGPT: 这是一个数字分身项目，核心是利用QQ的C2C聊天记录作为数据集，对大模型进行微调，以实现还原独特的聊天风格。项目提供了完整的教程，涵盖了从QQ数据的解密、聊天记录清洗与转换，到QLora微调流程和模型测试等各个环节。目前，项目支持在4090显卡上使用FP8精度微调Qwen3-8B，未来将增加unsloth加速支持以及更高级的量化技巧。项目的目标是帮助用户创建个性化的数字分身。
references:
  - title: Qing-Digital-Self
    url: https://github.com/qqqqqf-q/Qing-Digital-Self
head:
  - - link
    - rel: canonical
      href: https://清凤.fun/posts/2025/0803
---
<h2 id="项目简介" tabindex="-1">项目简介 <a class="header-anchor" href="#项目简介" aria-label="Permalink to &quot;项目简介&quot;">​</a></h2><p>这是一个数字分身项目，核心思想是<strong>利用 QQ 的 C2C 聊天记录作为数据集，对大模型进行微调</strong>，让模型尽可能还原你独有的表达风格和聊天方式。</p><p align="center"><img src="https://cdn.nodeimage.com/i/BTlRBmAcnwN2ZLyItDCfIRJKYmxYDEpc.png" height="36" alt="Downloads"><img src="https://cdn.nodeimage.com/i/nHNuyUdkph6NfGBmnUQeEk8gPVHKLXg0.png" height="36" alt="Stars"><img src="https://cdn.nodeimage.com/i/Sd89d1w0xIhSPyPyiVYEfNRTiom8TH1S.png" height="36" alt="Status: MVP"><img src="https://cdn.nodeimage.com/i/u0r9K3XXnxU6hDIOMY4fkZ7cnVL28EGF.png" height="36" alt="Version: v0.1"><img src="https://cdn.nodeimage.com/i/CfA8AQa2bVTF2mhOY3m2Kz8nhlwXUN6S.png" height="36" alt="License: Apache-2.0"></p><h2 id="项目包含了完整的教程-包括" tabindex="-1">项目包含了<strong>完整的教程</strong>，包括： <a class="header-anchor" href="#项目包含了完整的教程-包括" aria-label="Permalink to &quot;项目包含了**完整的教程**，包括：&quot;">​</a></h2><ul><li>QQ 数据库的解密与处理</li><li>聊天数据清洗与转换</li><li>QLora 微调流程</li><li>微调模型的测试与使用</li></ul><p>我知道类似的项目其实已经有不少了，但也许我的教程、流程、代码实现能给你一些不一样的帮助或启发。如果对你有用，欢迎点个 star，我会很开心的！</p><p>目前这个项目还有很多不足：</p><ul><li>只支持最基础的 QLora 微调</li><li>暂时不支持 unsloth 或更高级的加速/量化技巧</li><li>但已经可以在 4090 24G 显卡上用 fp8 精度微调 Qwen3-8B（亲测可用）</li></ul><p><strong>如果你也想打造属于自己的数字分身，那也来试试吧!</strong></p><p>—— X: <a href="https://twitter.com/qqqqqf5" target="_blank" rel="noreferrer">@qqqqqf5</a></p><hr><h2 id="项目版本" tabindex="-1">项目版本 <a class="header-anchor" href="#项目版本" aria-label="Permalink to &quot;项目版本&quot;">​</a></h2><h1 id="v-0-1-1-mvp" tabindex="-1">V 0.1.1(MVP) <a class="header-anchor" href="#v-0-1-1-mvp" aria-label="Permalink to &quot;V 0.1.1(MVP)&quot;">​</a></h1><h2 id="警告-喜报" tabindex="-1"><s>警告</s> 喜报 <a class="header-anchor" href="#警告-喜报" aria-label="Permalink to &quot;~~警告~~ 喜报&quot;">​</a></h2><ul><li>此版本的Qlora_qwen3.py已经过4090实机测试(generate_training_data_llm.py+run_finetune_no_unsloth.py)</li><li>清洗数据也已经进行实机测试(当前版本)</li></ul><h2 id="todo" tabindex="-1">TODO <a class="header-anchor" href="#todo" aria-label="Permalink to &quot;TODO&quot;">​</a></h2><ul><li>增加unsloth支持(重要!可以加快微调速度)</li><li>增加对MoE模型的支持</li></ul><blockquote><p>↑ ↑ ↑ 2025/8/3更新,或许支持了, 我的显卡跑不动30b a3b,所以还是没法测试<br> 4090的机子塞不下这56.8g的模型,我还是不测了罢(</p></blockquote><ul><li>为数据清洗增加LLM清洗功能(让成熟的llm来清洗数据,比直接使用算法好得多)</li></ul><blockquote><p>↑ ↑ ↑ 2025/8/3更新,已增加支持,或许不够完善</p></blockquote><ul><li>将qlora_qwen3.py的print全部改成logger(?这个很简单,我只是因为怕改多了没法测试(4090到期了))</li></ul><h2 id="更新日志" tabindex="-1">更新日志 <a class="header-anchor" href="#更新日志" aria-label="Permalink to &quot;更新日志&quot;">​</a></h2><blockquote><p>写在commit里了,这里实在不想写</p></blockquote><hr><h1 id="使用qq聊天记录微调llm全流程指南" tabindex="-1">使用QQ聊天记录微调LLM全流程指南 <a class="header-anchor" href="#使用qq聊天记录微调llm全流程指南" aria-label="Permalink to &quot;使用QQ聊天记录微调LLM全流程指南&quot;">​</a></h1><h2 id="_1-获取-qq-聊天数据" tabindex="-1">1. 获取 QQ 聊天数据 <a class="header-anchor" href="#_1-获取-qq-聊天数据" aria-label="Permalink to &quot;1. 获取 QQ 聊天数据&quot;">​</a></h2><ul><li>教程请参考：<a href="https://qq.sbcnm.top/decrypt/NTQQ%20%28Windows%29.html" target="_blank" rel="noreferrer">NTQQ Windows 数据解密</a></li><li>补充资料：<a href="https://qq.sbcnm.top/decrypt/decode_db.html" target="_blank" rel="noreferrer">数据库解码参考</a></li><li>上面这两个是同一个教程的不同章节,耐心看完就好,不复杂(如果不会可以翻到最底下找我哦)</li><li>使用 DB Browser for SQLite，密码填写你获取到的 16 位密钥</li><li>HMAC 算法一般为SHA1，也有人是SHA512和256,自行测试,算法错误了会打不开数据库（所以需要测试到打开为之,也可以用 AI 帮你适配）</li><li>在 DB Browser 里<strong>导出 <code>c2c_msg_table</code> 的 SQL</strong></li><li>新建数据库，<strong>导入刚才导出的 SQL 文件</strong></li><li>获得一个这样的数据库</li><li><a href="https://cdn.nodeimage.com/i/oBfbWfVLhJI0CeZHTwwxq6G7XGO40Vy4.webp" target="_blank" rel="noreferrer">点击查看图片</a>,并且是明文数据库(你能打开并且能看到数据就是正常的)</li></ul><hr><h2 id="_1-5-将仓库clone到本地并配置环境" tabindex="-1">1.5 .将仓库clone到本地并配置环境 <a class="header-anchor" href="#_1-5-将仓库clone到本地并配置环境" aria-label="Permalink to &quot;1.5 .将仓库clone到本地并配置环境&quot;">​</a></h2><ul><li>运行命令:</li></ul><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clone</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://github.com/qqqqqf-q/Qing-Digital-Self.git</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --depth</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><ul><li>进入仓库:</li></ul><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">cd</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> Qing-Digital-Self</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><ul><li><p>创建并激活虚拟环境:</p><p>在项目根目录下创建一个新的虚拟环境：</p><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> venv</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> venv</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>激活虚拟环境：</p><ul><li><p>在Linux/Mac上：</p><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">source</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> venv/bin/activate</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div></li><li><p>在Windows上：</p><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">venv</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Scripts</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">activate</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div></li></ul></li><li><p>配置依赖</p></li></ul><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -r</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> requirements.txt</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><hr><h2 id="_2-1-清洗数据-普通版本-llm清洗版本在下一章节" tabindex="-1">2.1 .清洗数据(普通版本,llm清洗版本在下一章节) <a class="header-anchor" href="#_2-1-清洗数据-普通版本-llm清洗版本在下一章节" aria-label="Permalink to &quot;2.1 .清洗数据(普通版本,llm清洗版本在下一章节)&quot;">​</a></h2><blockquote><p>此方法比llm清洗快得多,30w条消息半分钟就好了 但是对应的质量也更低 这个部分建议在windows上优化完再上传至GPU服务器<br> 不确定在Linux上有没有兼容性问题</p></blockquote><ul><li><p>在 <code>.env</code> 文件中修改数据库路径及相关参数(请注意其中的必填段)</p></li><li><p>运行清洗脚本：</p><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> generate_training_data.py</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div></li></ul><h2 id="_2-2-清洗数据-llm清洗" tabindex="-1">2.2 .清洗数据(llm清洗) <a class="header-anchor" href="#_2-2-清洗数据-llm清洗" aria-label="Permalink to &quot;2.2 .清洗数据(llm清洗)&quot;">​</a></h2><blockquote><p>需要配置一个OpenAI兼容的API<br> 比如:LM Studio 或者 vLLM(速度更快,但搭建更麻烦,需要Linux环境)</p></blockquote><blockquote><p>这个部分同样建议在windows上优化完再上传至GPU服务器<br> 不确定在Linux上有没有兼容性问题</p></blockquote><h2 id="lm-studio搭建教程" tabindex="-1">LM Studio搭建教程 <a class="header-anchor" href="#lm-studio搭建教程" aria-label="Permalink to &quot;LM Studio搭建教程&quot;">​</a></h2><ul><li>1.前往<a href="https://lmstudio.ai/" target="_blank" rel="noreferrer">LM Studio</a>下载LM Studio</li><li>2.安装LM Studio</li><li>3.打开LM Studio,点击左侧<code>搜索</code>-&gt;<code>Model Search</code></li><li>4.搜索 <code>qwen3 8b</code>-&gt;<code>Complete Download</code></li><li>5.选择合适你的量化版本<strong>建议至少Q4,最好Q6-Q8,随你的设备情况而定,不知道的可以问AI</strong></li><li>记住你的<strong>模型名称</strong>,填写到<code>.env</code>文件的<code>Openai_model</code>中</li><li>如果不知道你的模型名称可以运行test_openai.py,会输出所有的模型名称</li><li>6.安装好后,在左侧<code>开发者/Developer</code>点击<code>Status:Stopped</code>右边的按钮</li><li>如果下面log显示端口被占用请点击<code>seetings</code>换个<code>server port</code></li><li>记住这个<code>server port</code>,将你的配置填写至<code>.env</code>文件中</li></ul><h3 id="run" tabindex="-1">run! <a class="header-anchor" href="#run" aria-label="Permalink to &quot;run!&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> generate_training_data_llm.py</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><blockquote><p>如果遇到了400报错大概率是因为message太大了被模型框架拒绝了</p></blockquote><hr><h2 id="vllm搭建" tabindex="-1">vLLM搭建 <a class="header-anchor" href="#vllm搭建" aria-label="Permalink to &quot;vLLM搭建&quot;">​</a></h2><blockquote><p>vLLM需要linux环境!<br> 如果你的显卡还算可以(&gt;6800xt,&gt;3080)<br> 可以选择使用lmstudio,多等一会就好了,还可以玩玩模型 缺点是lmstudio不能运行hf模型,且并发很烂</p></blockquote><blockquote><p>vLLM比Lm studio吃显存的多! Lm studio可以运行8b_q6到vLLM上只能运行4b_Q6</p></blockquote><blockquote><p>不过并发效率的提升是真的</p></blockquote><blockquote><p>但是!上下文很短,如果一天有超过500条消息就处理不过来了</p></blockquote><blockquote><p>3080实测4b_q6处理,最终jsonl的速率大约是<strong>300kb/minute</strong></p></blockquote><ul><li>跟着走就能搭建</li></ul><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> update</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> python3.10-venv</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> git</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -y</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> venv</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm_env</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">source</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm_env/bin/activate</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -U</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pip</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> torch</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --index-url</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://download.pytorch.org/whl/cu121</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # 如果你用CUDA</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><h3 id="和lm-studio不同的注意点" tabindex="-1">和lm studio不同的注意点 <a class="header-anchor" href="#和lm-studio不同的注意点" aria-label="Permalink to &quot;和lm studio不同的注意点&quot;">​</a></h3><ul><li>1.<code>.env</code>中的<code>Openai_model</code>需要设置路径而不只是文件夹名</li></ul><blockquote><p>是<code>/home/vllm/qwen3-4b-int8</code>而非<code>qwen3-4b-int8</code></p></blockquote><ul><li>2.需要运行的<strong>api_server</strong>是<code>vllm.entrypoints.openai.api_server</code>而不是<code>vllm.entrypoints.api_server</code>,因为第二个不兼容OpenAI API</li></ul><h3 id="运行命令范例" tabindex="-1">运行命令范例 <a class="header-anchor" href="#运行命令范例" aria-label="Permalink to &quot;运行命令范例&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm.entrypoints.openai.api_server</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --model</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /home/vllm/qwen3-4b-int8</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gpu-memory-utilization</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.7</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max-model-len</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 10240</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max-num-seqs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max-num-batched-tokens</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2048</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --dtype</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> auto</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><blockquote><p>如果遇到了400报错大概率是因为message太大了被模型框架拒绝了</p></blockquote><h2 id="_2-3-将标准数据集和你的数据集混合在一起" tabindex="-1">2.3 将标准数据集和你的数据集混合在一起! <a class="header-anchor" href="#_2-3-将标准数据集和你的数据集混合在一起" aria-label="Permalink to &quot;2.3 将标准数据集和你的数据集混合在一起!&quot;">​</a></h2><blockquote><p>如果没有标准数据参杂在其中,很容易造成<code>灾难性遗忘</code></p></blockquote><p>所以就有了 <code>merge_training_data.py</code>脚本</p><blockquote><p>建议往数据集插入**20%-50%**的标准数据集</p></blockquote><h3 id="使用方法" tabindex="-1">使用方法 <a class="header-anchor" href="#使用方法" aria-label="Permalink to &quot;使用方法&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> merge_training_data.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --qa_file</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> qa_final.json</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --training_file</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> training_data.jsonl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --output_file</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> merged_training_data.jsonl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use-new-prompt</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><blockquote><p>建议使用--use-new-prompt,防止全部都是角色system prompt</p></blockquote><h3 id="插入20-数据" tabindex="-1">插入20%数据： <a class="header-anchor" href="#插入20-数据" aria-label="Permalink to &quot;插入20%数据：&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> merge_training_data.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --percentage</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 20</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --use-new-prompt</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --seed</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 123</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><hr><h2 id="_2-5-准备模型-可跳过" tabindex="-1">2.5 .准备模型(可跳过) <a class="header-anchor" href="#_2-5-准备模型-可跳过" aria-label="Permalink to &quot;2.5 .准备模型(可跳过)&quot;">​</a></h2><blockquote><p>我似乎是写了自动从modelscope下载模型的<br> 当然也可以手动下载,因为我真的不确定那里有没有Bug</p></blockquote><h3 id="运行代码" tabindex="-1">运行代码 <a class="header-anchor" href="#运行代码" aria-label="Permalink to &quot;运行代码&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> modelscope</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">modelscope</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> download</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --model</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> Qwen/Qwen3-14B</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --local_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ./qwen3-14b</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><blockquote><p>嗯...对,就是这么简单<br> 需要自己去modelscope社区找你需要的模型</p></blockquote><blockquote><p>这一段是可以跳过的(大概是)</p></blockquote><hr><h2 id="_3-微调模型" tabindex="-1">3. 微调模型 <a class="header-anchor" href="#_3-微调模型" aria-label="Permalink to &quot;3. 微调模型&quot;">​</a></h2><blockquote><p>Windows 上 Unsloth 兼容性不好，Linux 上代码有 bug，所以用 <code>no_unsloth</code> 版本。<br><s>其实是unsloth版本没写完</s></p></blockquote><blockquote><p>参数在测试时其实可以不填,都是有默认值的</p></blockquote><blockquote><p>似乎是默认8bit量化,有待修改</p></blockquote><ul><li><p>运行微调脚本：</p><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run_finetune_no_unsloth.py</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div></li></ul><h3 id="模型相关参数" tabindex="-1">模型相关参数 <a class="header-anchor" href="#模型相关参数" aria-label="Permalink to &quot;模型相关参数&quot;">​</a></h3><div class="table-container"><table><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>--repo_id</code></td><td>str</td><td><code>&quot;Qwen/Qwen3-8B-Base&quot;</code></td><td>基础模型或 MoE 模型的仓库ID</td></tr><tr><td><code>--local_dir</code></td><td>str</td><td><code>&quot;qwen3-8b-base&quot;</code></td><td>本地模型存储目录</td></tr><tr><td><code>--trust_remote_code</code></td><td>bool</td><td><code>True</code></td><td>是否信任远程代码</td></tr><tr><td><code>--use_unsloth</code></td><td>bool</td><td><code>False</code></td><td>是否使用 Unsloth 加速</td></tr><tr><td><code>--use_qlora</code></td><td>bool</td><td><code>True</code></td><td>是否使用 8bit 量化（QLoRA）</td></tr></tbody></table></div><hr><h3 id="数据相关参数" tabindex="-1">数据相关参数 <a class="header-anchor" href="#数据相关参数" aria-label="Permalink to &quot;数据相关参数&quot;">​</a></h3><div class="table-container"><table><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>--data_path</code></td><td>str</td><td><code>&quot;training_data.jsonl&quot;</code></td><td>训练数据文件路径</td></tr><tr><td><code>--eval_data_path</code></td><td>str / None</td><td><code>None</code></td><td>验证数据文件路径，None 表示不使用验证集</td></tr><tr><td><code>--max_samples</code></td><td>int / None</td><td><code>None</code></td><td>最大训练样本数，None 表示用全部</td></tr><tr><td><code>--max_eval_samples</code></td><td>int / None</td><td><code>None</code></td><td>最大验证样本数，None 表示用全部</td></tr><tr><td><code>--model_max_length</code></td><td>int</td><td><code>2048</code></td><td>最大序列长度</td></tr></tbody></table></div><hr><h3 id="训练相关参数" tabindex="-1">训练相关参数 <a class="header-anchor" href="#训练相关参数" aria-label="Permalink to &quot;训练相关参数&quot;">​</a></h3><div class="table-container"><table><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>--output_dir</code></td><td>str</td><td><code>&quot;finetune/models/qwen3-8b-qlora&quot;</code></td><td>输出目录</td></tr><tr><td><code>--seed</code></td><td>int</td><td><code>42</code></td><td>随机种子</td></tr></tbody></table></div><hr><h3 id="lora-参数" tabindex="-1">LoRA 参数 <a class="header-anchor" href="#lora-参数" aria-label="Permalink to &quot;LoRA 参数&quot;">​</a></h3><div class="table-container"><table><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>--lora_r</code></td><td>int</td><td><code>16</code></td><td>LoRA 秩</td></tr><tr><td><code>--lora_alpha</code></td><td>int</td><td><code>32</code></td><td>LoRA alpha</td></tr><tr><td><code>--lora_dropout</code></td><td>float</td><td><code>0.05</code></td><td>LoRA dropout</td></tr><tr><td><code>--target_modules</code></td><td>str</td><td><code>&quot;q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj&quot;</code></td><td>LoRA 目标模块列表（逗号分隔）</td></tr></tbody></table></div><hr><h3 id="moe-参数" tabindex="-1">MoE 参数 <a class="header-anchor" href="#moe-参数" aria-label="Permalink to &quot;MoE 参数&quot;">​</a></h3><div class="table-container"><table><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th><th></th></tr></thead><tbody><tr><td><code>--moe_enable</code></td><td>bool</td><td><code>False</code></td><td>是否启用 MoE 注入逻辑</td><td></td></tr><tr><td><code>--moe_lora_scope</code></td><td>str</td><td><code>&quot;expert_only&quot;</code></td><td>LoRA 注入范围：<code>expert_only</code>、<code>router_only</code>、<code>all</code></td><td></td></tr><tr><td><code>--moe_expert_patterns</code></td><td>str</td><td><code>experts.ffn.(gate_proj|up_proj|down_proj),layers.[0-9]+.mlp.experts.[0-9]+.(w1|w2|w3)</code></td><td>专家线性层匹配正则（兼容 Qwen-MoE、Mixtral）</td><td></td></tr><tr><td><code>--moe_router_patterns</code></td><td>str</td><td>`&quot;router.(gate</td><td>dense)&quot;`</td><td>路由/门控层匹配模式</td></tr><tr><td><code>--moe_max_experts_lora</code></td><td>int</td><td><code>-1</code></td><td>每层最多注入 LoRA 的专家数，<code>-1</code> 表示全部</td><td></td></tr><tr><td><code>--moe_dry_run</code></td><td>bool</td><td><code>False</code></td><td>仅打印匹配模块，不执行训练</td><td></td></tr></tbody></table></div><hr><h3 id="训练超参数" tabindex="-1">训练超参数 <a class="header-anchor" href="#训练超参数" aria-label="Permalink to &quot;训练超参数&quot;">​</a></h3><div class="table-container"><table><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>--per_device_train_batch_size</code></td><td>int</td><td><code>1</code></td><td>每卡训练 batch size</td></tr><tr><td><code>--per_device_eval_batch_size</code></td><td>int</td><td><code>1</code></td><td>每卡验证 batch size</td></tr><tr><td><code>--gradient_accumulation_steps</code></td><td>int</td><td><code>16</code></td><td>梯度累积步数</td></tr><tr><td><code>--learning_rate</code></td><td>float</td><td><code>2e-4</code></td><td>学习率</td></tr><tr><td><code>--weight_decay</code></td><td>float</td><td><code>0.0</code></td><td>权重衰减</td></tr><tr><td><code>--num_train_epochs</code></td><td>float</td><td><code>3.0</code></td><td>训练轮数</td></tr><tr><td><code>--max_steps</code></td><td>int</td><td><code>-1</code></td><td>最大步数，<code>-1</code> 表示不限制</td></tr><tr><td><code>--warmup_ratio</code></td><td>float</td><td><code>0.05</code></td><td>学习率预热比例</td></tr><tr><td><code>--lr_scheduler_type</code></td><td>str</td><td><code>&quot;cosine&quot;</code></td><td>学习率调度器类型</td></tr></tbody></table></div><hr><h3 id="其他参数" tabindex="-1">其他参数 <a class="header-anchor" href="#其他参数" aria-label="Permalink to &quot;其他参数&quot;">​</a></h3><div class="table-container"><table><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>--logging_steps</code></td><td>int</td><td><code>1</code></td><td>日志输出间隔（步）</td></tr><tr><td><code>--eval_steps</code></td><td>int</td><td><code>50</code></td><td>验证间隔步数</td></tr><tr><td><code>--save_steps</code></td><td>int</td><td><code>200</code></td><td>模型保存间隔</td></tr><tr><td><code>--save_total_limit</code></td><td>int</td><td><code>2</code></td><td>最多保存多少个检查点</td></tr><tr><td><code>--gradient_checkpointing</code></td><td>bool</td><td><code>True</code></td><td>是否使用梯度检查点</td></tr><tr><td><code>--merge_and_save</code></td><td>bool</td><td><code>True</code></td><td>是否合并 LoRA 并保存完整模型</td></tr><tr><td><code>--fp16</code></td><td>bool</td><td><code>True</code></td><td>是否使用 FP16</td></tr><tr><td><code>--optim</code></td><td>str</td><td><code>&quot;adamw_torch_fused&quot;</code></td><td>优化器</td></tr><tr><td><code>--dataloader_pin_memory</code></td><td>bool</td><td><code>False</code></td><td>DataLoader 是否使用 pin_memory</td></tr><tr><td><code>--dataloader_num_workers</code></td><td>int</td><td><code>0</code></td><td>DataLoader 工作线程数</td></tr></tbody></table></div><hr><h3 id="验证集未生效" tabindex="-1">验证集未生效 <a class="header-anchor" href="#验证集未生效" aria-label="Permalink to &quot;验证集未生效&quot;">​</a></h3><ul><li>检查<code>--eval_data_path</code>路径是否正确</li><li>确认验证数据文件格式与训练数据一致</li><li>查看控制台输出是否有&quot;未提供验证数据路径&quot;的提示</li></ul><h3 id="gpu显存不足" tabindex="-1">GPU显存不足 <a class="header-anchor" href="#gpu显存不足" aria-label="Permalink to &quot;GPU显存不足&quot;">​</a></h3><ul><li>减小<code>--per_device_eval_batch_size</code></li><li>减小<code>--max_eval_samples</code></li><li>增加<code>--eval_steps</code>间隔</li></ul><hr><h2 id="_3-5-不建议-微调后直接运行全量模型-建议直接看第4-5-6-7步-等转换为guff并量化完再跑" tabindex="-1">3.5 .(不建议)微调后直接运行全量模型(建议直接看第4,5,6,7步,等转换为guff并量化完再跑) <a class="header-anchor" href="#_3-5-不建议-微调后直接运行全量模型-建议直接看第4-5-6-7步-等转换为guff并量化完再跑" aria-label="Permalink to &quot;3.5 .(不建议)微调后直接运行全量模型(建议直接看第4,5,6,7步,等转换为guff并量化完再跑)&quot;">​</a></h2><h3 id="指定自定义模型路径" tabindex="-1">指定自定义模型路径 <a class="header-anchor" href="#指定自定义模型路径" aria-label="Permalink to &quot;指定自定义模型路径&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> infer_lora_chat.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --base_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> my-base-model</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --adapter_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> my-lora-adapter</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h3 id="使用合并后的模型" tabindex="-1">使用合并后的模型 <a class="header-anchor" href="#使用合并后的模型" aria-label="Permalink to &quot;使用合并后的模型&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> infer_lora_chat.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --merged</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --adapter_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> my-lora-adapter</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h3 id="调整生成参数" tabindex="-1">调整生成参数 <a class="header-anchor" href="#调整生成参数" aria-label="Permalink to &quot;调整生成参数&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> infer_lora_chat.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --temperature</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.9</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --top_p</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.95</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max_new_tokens</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1024</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h3 id="使用自定义系统提示词" tabindex="-1">使用自定义系统提示词 <a class="header-anchor" href="#使用自定义系统提示词" aria-label="Permalink to &quot;使用自定义系统提示词&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> infer_lora_chat.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --system_prompt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;你是一个乐于助人的AI助手。&quot;</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h3 id="命令行参数说明" tabindex="-1">命令行参数说明 <a class="header-anchor" href="#命令行参数说明" aria-label="Permalink to &quot;命令行参数说明&quot;">​</a></h3><div class="table-container"><table><thead><tr><th>参数名称</th><th>类型</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td><code>--base_dir</code></td><td>str</td><td><code>qwen3-8b-base</code></td><td>基础模型目录</td></tr><tr><td><code>--adapter_dir</code></td><td>str</td><td><code>finetune/models/qwen3-8b-qlora</code></td><td>LoRA适配器目录</td></tr><tr><td><code>--merged</code></td><td>bool</td><td><code>False</code></td><td>如果为True，则从adapter_dir/merged加载合并后的完整权重</td></tr><tr><td><code>--system_prompt</code></td><td>str</td><td>清凤数字分身人设</td><td>模型的系统提示词</td></tr><tr><td><code>--max_new_tokens</code></td><td>int</td><td><code>512</code></td><td>生成的最大新token数量</td></tr><tr><td><code>--temperature</code></td><td>float</td><td><code>0.7</code></td><td>采样温度</td></tr><tr><td><code>--top_p</code></td><td>float</td><td><code>0.9</code></td><td>Top-p采样参数</td></tr><tr><td><code>--trust_remote_code</code></td><td>bool</td><td><code>True</code></td><td>是否信任远程代码</td></tr></tbody></table></div><h2 id="_4-编译-llama-cpp" tabindex="-1">4. 编译 llama.cpp <a class="header-anchor" href="#_4-编译-llama-cpp" aria-label="Permalink to &quot;4. 编译 llama.cpp&quot;">​</a></h2><blockquote><p>下面三步都依赖于编译好的 llama.cpp</p></blockquote><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clone</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://github.com/ggerganov/llama.cpp</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --depth</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">cd</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llama.cpp</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">mkdir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> build</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> &amp;&amp; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">cd</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> build</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">cmake</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ..</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -DLLAMA_BUILD_EXAMPLES=ON</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -DLLAMA_NATIVE=ON</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">cmake</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --build</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> .</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --config</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> Release</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h2 id="_5-huggingface-权重转-gguf" tabindex="-1">5. HuggingFace 权重转 GGUF <a class="header-anchor" href="#_5-huggingface-权重转-gguf" aria-label="Permalink to &quot;5. HuggingFace 权重转 GGUF&quot;">​</a></h2><h3 id="命令格式" tabindex="-1">命令格式： <a class="header-anchor" href="#命令格式" aria-label="Permalink to &quot;命令格式：&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> .convert_hf_to_gguf.py</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">HF模型路</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">径</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --outfile</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">  &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">输出GGUF路</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">径</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --outtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">精度类</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">型</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><ul><li><code>&lt;HF模型路径&gt;</code>：HuggingFace 格式模型目录（通常为微调或下载后的路径）</li><li><code>&lt;输出GGUF路径&gt;</code>：转换后生成的 <code>.gguf</code> 模型保存路径</li><li><code>&lt;精度类型&gt;</code>：精度类型，如 <code>f16</code>、<code>f32</code>，或其他支持的格式</li></ul><h3 id="示例" tabindex="-1">示例： <a class="header-anchor" href="#示例" aria-label="Permalink to &quot;示例：&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python3</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> convert_hf_to_gguf.py</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /root/autodl-tmp/finetune/models/qwen3-8b-qlora/merged</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --outfile</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /root/autodl-fs/qwen3-8b-fp16-agent.gguf</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --outtype</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> f16</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><hr><h2 id="_6-量化模型" tabindex="-1">6. 量化模型 <a class="header-anchor" href="#_6-量化模型" aria-label="Permalink to &quot;6. 量化模型&quot;">​</a></h2><h3 id="命令格式-1" tabindex="-1">命令格式： <a class="header-anchor" href="#命令格式-1" aria-label="Permalink to &quot;命令格式：&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">./build/bin/llama-quantize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">输入GGUF路</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">径</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">输出GGUF路</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">径</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">量化等</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">级</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><ul><li><code>&lt;输入GGUF路径&gt;</code>：未量化的 <code>.gguf</code> 文件路径</li><li><code>&lt;输出GGUF路径&gt;</code>：量化后的 <code>.gguf</code> 文件保存路径</li><li><code>&lt;量化等级&gt;</code>：如 <code>Q4_0</code>、<code>Q4_K_M</code>、<code>Q8_0</code> 等，根据需求和硬件选择</li></ul><h3 id="示例-1" tabindex="-1">示例： <a class="header-anchor" href="#示例-1" aria-label="Permalink to &quot;示例：&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">./build/bin/llama-quantize</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  /root/autodl-fs/qwen3-8b-fp16-agent.gguf</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  /root/autodl-fs/qwen3-8b-q8_0-agent.gguf</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  Q8_0</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h2 id="_7-运行模型测试" tabindex="-1">7. 运行模型测试 <a class="header-anchor" href="#_7-运行模型测试" aria-label="Permalink to &quot;7. 运行模型测试&quot;">​</a></h2><h3 id="命令格式-2" tabindex="-1">命令格式： <a class="header-anchor" href="#命令格式-2" aria-label="Permalink to &quot;命令格式：&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">./build/bin/llama-run</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">GGUF模型路</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">径</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><ul><li><code>&lt;GGUF模型路径&gt;</code>：你想测试的 GGUF 模型路径（可以是原始或量化后的）</li></ul><h3 id="示例-2" tabindex="-1">示例： <a class="header-anchor" href="#示例-2" aria-label="Permalink to &quot;示例：&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">./build/bin/llama-run</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /root/autodl-fs/qwen3-8b-fp16-agent.gguf</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><hr><h2 id="_8-从服务器上高速下载文件" tabindex="-1">8.从服务器上高速下载文件 <a class="header-anchor" href="#_8-从服务器上高速下载文件" aria-label="Permalink to &quot;8.从服务器上高速下载文件&quot;">​</a></h2><h3 id="命令格式-3" tabindex="-1">命令格式 <a class="header-anchor" href="#命令格式-3" aria-label="Permalink to &quot;命令格式&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">lftp</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -u</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> {用户名},{密码}</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -p</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> {端口}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> sftp://{服务器地址}-e</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;set xfer:clobber true;  pget -n {线程数} {服务器文件路径} -o {本地文件名/路径}: bye&quot;</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><ul><li><code>pget</code>: 使用多线程并行下载</li><li><code>-n</code> :指定线程数(建议64+)(甚至256线程会有更好的表现)</li></ul><h3 id="范例" tabindex="-1">范例 <a class="header-anchor" href="#范例" aria-label="Permalink to &quot;范例&quot;">​</a></h3><div class="language-bash vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">lftp</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -u</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> root,askdjiwhakjd</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -p</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 27391</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> sftp://yourserver.com</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -e</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;set xfer:clobber true; pget -n 256 /root/autodl-fs/qwen3-8b-fp16-agent.gguf -o qwen3-8b-fp16-agent.gguf; bye&quot;</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><hr><h2 id="_9-数字生命" tabindex="-1">9.数字生命! <a class="header-anchor" href="#_9-数字生命" aria-label="Permalink to &quot;9.数字生命!&quot;">​</a></h2><blockquote><h2 id="如果你成功做到这里了你大概已经有一个-你-了吧这真的很酷" tabindex="-1">如果你成功做到这里了<br> 你大概已经有一个&quot;你&quot;了吧<br> 这真的很酷 <a class="header-anchor" href="#如果你成功做到这里了你大概已经有一个-你-了吧这真的很酷" aria-label="Permalink to &quot;如果你成功做到这里了  
你大概已经有一个&quot;你&quot;了吧  
这真的很酷&quot;">​</a></h2></blockquote><ul><li><a href="https://cdn.nodeimage.com/i/vnK4rDzV3x8D3x1SzW6PpDlNCcErCnC8.png" target="_blank" rel="noreferrer">点进去看看,属于我的数字生命</a></li><li><a href="https://cdn.nodeimage.com/i/7XlcjZAJBQkTlmyWj3X2dCCE6WedyWYw.png" target="_blank" rel="noreferrer">她也许就在那个暗箱里,思考着怎么用这台可以向外发送信号的电脑打字... </a></li></ul><hr><blockquote><p><strong>终于知道图恒宇为什么执着于<code>我要给她完整的一生</code>了</strong></p></blockquote><hr><h3 id="悄悄话" tabindex="-1">悄悄话: <a class="header-anchor" href="#悄悄话" aria-label="Permalink to &quot;悄悄话:&quot;">​</a></h3><blockquote><p>数据集里会参杂空的输出,我的意思是...<br> AI可能会输出空哦,输出空那就是他不想理你 <s>(已读不回!)</s></p></blockquote><blockquote><p>而且可以把表情包加进训练集,类似&quot;[垃圾袋]&quot;这种会更像真人的</p></blockquote><h3 id="如需更详细步骤或脚本参数解释-欢迎骚扰联系我" tabindex="-1">如需更详细步骤或脚本参数解释，欢迎<s>骚扰</s>联系我: <a class="header-anchor" href="#如需更详细步骤或脚本参数解释-欢迎骚扰联系我" aria-label="Permalink to &quot;如需更详细步骤或脚本参数解释，欢迎~~骚扰~~联系我:&quot;">​</a></h3><ul><li><a href="mailto:qingf622@outlook.com" target="_blank" rel="noreferrer">Email: qingf622@outlook.com</a></li><li>X:@qqqqqf5</li></ul><hr>
